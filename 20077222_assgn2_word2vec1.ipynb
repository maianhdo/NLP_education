{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "varying-publicity",
   "metadata": {},
   "source": [
    "### <center> Assignment 2: word2vec </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "entire-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "middle-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#in this function we replace some symbols like \"(, ), [, ], {, }, |, \\, /, ','\" etc by space\n",
    "#DO NOT REMOVE STOPWORDS FROM TEXT\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile(r'[/(){}\\[\\]\\|@,;\\.:\\->_\\?\\'\\\"\\*\\d+]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def prepare_txt(text):\n",
    "    text = REPLACE_BY_SPACE_RE.sub(\" \", text)\n",
    "    text = re.sub(\"  \", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "laden-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_train_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = prepare_txt(text).lower()\n",
    "    return text\n",
    "text = create_raw_train_data(\"news_dataset1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-conspiracy",
   "metadata": {},
   "source": [
    "#### <center> word2vec summary  </center>\n",
    "1. Set up a fake problem of determining the context words from a target word *w*.    \n",
    "2. The training set is the text or collection of texts. \n",
    "3. How to determine the context words of a fixed target is word is already explained. Note that a target word may appear multiple times.   \n",
    "4. If *w* is the target word and *c* is a context word the basic skip-gram model uses the formula\n",
    "$$ p(c|w) = \\frac{\\exp{({\\mathbf v}_c\\cdot {\\mathbf v}_w)}} {\\sum_{w' \\in V}\\exp{({\\mathbf v}_{w'}\\cdot {\\mathbf v}_w)}} $$  \n",
    "Here, ${\\mathbf v}_{w'}$ is the vector representing the word $w'$ and similarly for other words. So, the fake classification task is actually determine these vectors from the weights assigned to the network when we solve the classification problem.    \n",
    "5. There are two main practical issues we have to deal with. The first is that the denominator in the above equation requires that we sum over all words in teh vocabulary, which is usually quite large. The second problem is that common words like 'a', \"the\" etc are in the \"window\" of most words. The most popular solutions are negative sampling and subsampling respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "random-carrier",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## First we need to compute individual word or unigram probabilities.\n",
    "## For this, we need word freqencies\n",
    "from collections import defaultdict\n",
    "#unigm_freq = defaultdict(int)\n",
    "##word list in order \n",
    "\n",
    "word_ls = text.split()\n",
    "def unigm_weight(ls):\n",
    "    f_dict = defaultdict(int)\n",
    "    for token in ls:\n",
    "        if token in f_dict:\n",
    "            f_dict[token] = f_dict[token]+  1\n",
    "        else: \n",
    "            f_dict[token] = 1 \n",
    "    return f_dict\n",
    "# unigm_freq0[\"the\"], len(word_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "toxic-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigm_freq0 = unigm_weight(word_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "assumed-strain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185, 2689)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigm_freq0['the'], len(word_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-ability",
   "metadata": {},
   "source": [
    "Answer= (185, 2689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "universal-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add the frequencies raised to the power .75\n",
    "def total_weight(f_dict):\n",
    "    freq_raised = defaultdict(int)\n",
    "    for word in f_dict:\n",
    "        freq_raised[word] =  f_dict[word]**3/4\n",
    "    wght =  np.sum(list(freq_raised.values()))\n",
    "    return  freq_raised, wght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "precise-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_raised, wght = total_weight(unigm_freq0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "broad-marketplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2118424.75"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wght"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-dating",
   "metadata": {},
   "source": [
    "**Explanation.**\n",
    "We want to calculate word or unigram probabilities. For a word *w* in vocabulary V, naively we will use \n",
    "$$ P(w) = \\frac{freq(w)}{\\sum_{w'\\in V} freq(w')} $$\n",
    "However, empirically it is found that a better estimate is given by the formula, \n",
    "$$ P(w) = \\frac{freq(w)^{3/4}}{\\sum_{w'\\in V} freq(w')^{3/4}} $$\n",
    "So, in the function we fill up the frequencies in a dictionary and compute the denominator, call it *wt*. We can now caluculate $P(w)$ for any word $w$ \"on-the-fly\" by looking up the word in the unigram frequency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-luxembourg",
   "metadata": {},
   "source": [
    "##### <center>Subsampling </center>\n",
    "Subsampling is done to get rid of some of the instances of frequently occuring words. For each word *w* in the vocabulary *V* let $z(w) = \\frac{freq(w)}{|V|} $, denote the relative frequency of the word. Then define, \n",
    "$$q(w) = \\min\\Big(1, \\big[\\sqrt{\\frac{z(w)}{t}} + 1 \\big]\\frac{t}{z(w)}\\Big) $$\n",
    "Here, $q(w)$ is the probability that the word *w* is kept and $t$ is threshold parameter. Take $t=.01$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "earlier-sleeve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3422599401936102"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability for subsampling, returns the function q for a numerical input\n",
    "N = len(word_ls)\n",
    "def prob_subsample(z):\n",
    "    t = 0.005\n",
    "    \n",
    "    y =  np.minimum(1,(np.sqrt((z/N)/t) + 1)*(t/(z/N)))\n",
    "    return y\n",
    "prob_subsample(185)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-consumer",
   "metadata": {},
   "source": [
    "Answer = .342 (approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "tight-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for subsampling. Use Bernoulli sampling to keep/remove words. \n",
    "from scipy.stats import bernoulli\n",
    "word_list2 = word_ls.copy()\n",
    "def subsample(lst):\n",
    "    unigm_freq1 = unigm_weight(lst)\n",
    "    lst_count = len(lst)\n",
    "    sub_ls = []\n",
    "    for word in word_list2:\n",
    "        prob_sub_sample = prob_subsample(unigm_freq1[word])\n",
    "        keep_value = bernoulli.rvs(prob_sub_sample, size = 1)\n",
    "        if keep_value == 1:\n",
    "            sub_ls.append(word)\n",
    "    unigm_freq1 = unigm_weight(sub_ls)\n",
    "    return sub_ls, unigm_freq1\n",
    "#You should also update the dictionary, unigm_freq, with new counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "diverse-metadata",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_list_sub, unigm_freq1 = subsample(word_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "novel-wednesday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(974, 974)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigm_freq0.keys()), len(unigm_freq1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-possession",
   "metadata": {},
   "source": [
    "Answer = (974, 974)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "unknown-hurricane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2420, 60, 185, 36, 62)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_sub), unigm_freq1[\"the\"],unigm_freq0[\"the\"], unigm_freq1[\"a\"], unigm_freq0[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-deviation",
   "metadata": {},
   "source": [
    "Answer = (2460, 77, 23), may change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "worldwide-bruce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(974, 974)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(unigm_freq1.keys())\n",
    "len(unigm_freq0.keys()), len(unigm_freq1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "executed-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = ['a', 'the', 'what', 'ho', 'my', 'go', 'brown', 'cow', 'a', 'the', 'deep', 'hi', 'laugh', 'the', 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "native-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word encoding\n",
    "def encode_word(voc):\n",
    "    w_ind = {voc[i]:i for i in range(len(voc))}\n",
    "    ind_w = {w_ind[x]:x for x in w_ind}\n",
    "\n",
    "    return w_ind, ind_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-looking",
   "metadata": {},
   "source": [
    "indx_word = {0: 'laugh',\n",
    " 1: 'to',\n",
    " 2: 'cow',\n",
    " 3: 'go',\n",
    " 4: 'my',\n",
    " 5: 'hi',\n",
    " 6: 'brown',\n",
    " 7: 'ho',\n",
    " 8: 'the',\n",
    " 9: 'what',\n",
    " 10: 'a',\n",
    " 11: 'deep'}, may be different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-greeting",
   "metadata": {},
   "source": [
    "word_indx = {'laugh': 0,\n",
    " 'to': 1,\n",
    " 'cow': 2,\n",
    " 'go': 3,\n",
    " 'my': 4,\n",
    " 'hi': 5,\n",
    " 'brown': 6,\n",
    " 'ho': 7,\n",
    " 'the': 8,\n",
    " 'what': 9,\n",
    " 'a': 10,\n",
    " 'deep': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "marked-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context window size = k\n",
    "def context(wls, k):\n",
    "    set_word = list(set(wls))\n",
    "    word_indx, indx_word = encode_word(set_word)\n",
    "    wls_len = len(wls)\n",
    "    c_dict = defaultdict(set)\n",
    "    for word in set_word:\n",
    "        word_idx = word_indx[word]\n",
    "        for i in range(len(wls)):\n",
    "            if word == wls[i]: \n",
    "                \n",
    "                context_word = []\n",
    "                #create list of neighbors index\n",
    "\n",
    "                neighbors = np.arange(i-k ,i+k+1)\n",
    "                neighbors = [n for n in neighbors if n >= 0]\n",
    "                neighbors = [n for n in neighbors if n < wls_len]\n",
    "                neighbors = [n for n in neighbors if n != i]\n",
    "                \n",
    "                for n in neighbors:\n",
    "                    context_word.append(wls[n])\n",
    "\n",
    "                context_indx = []\n",
    "                for context_w in context_word:\n",
    "                    context_word_indx = word_indx[context_w]\n",
    "                    context_indx.append(context_word_indx)\n",
    "\n",
    "\n",
    "                if word_idx in c_dict:\n",
    "                    c_dict[word_idx] = c_dict[word_idx]+ context_indx\n",
    "                else:\n",
    "                    c_dict[word_idx] = context_indx\n",
    "\n",
    "    for i in c_dict:\n",
    "        c_dict[i] = list(set(c_dict[i]))\n",
    "\n",
    "\n",
    "    return c_dict, word_indx, indx_word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "roman-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# di = context(ls, 2)\n",
    "# di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "outer-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dict,word_indx, indx_word  = context(word_list_sub, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "atmospheric-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_raised1, wght1 = total_weight(unigm_freq1)\n",
    "wordProb= defaultdict(int)\n",
    "for word in freq_raised1:\n",
    "    wordProb[word] = freq_raised1[word]/ wght1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "imposed-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "#should returm m negative samples\n",
    "def neg_sample(w, m):\n",
    "\n",
    "    # create a large array A with size 100000\n",
    "    A = np.zeros((100000, 1))\n",
    "    instances_inA = defaultdict(int)\n",
    "    # create a dictionary: the number of occurence of each word in A\n",
    "    for word in wordProb:\n",
    "        instances_inA[word] =  int(wordProb[word] * len(A))\n",
    "        \n",
    "    # fill words into A with respect to the number of occurence of each word\n",
    "    begin_indx = 0\n",
    "    for word in instances_inA:\n",
    "\n",
    "        occurence = instances_inA[word]\n",
    "        word_idx = word_indx[word]\n",
    "        end_indx = begin_indx + occurence\n",
    "        A[begin_indx:end_indx] = word_idx\n",
    "        begin_indx = end_indx + 1\n",
    "    \n",
    "    \n",
    "    #create negative samples list\n",
    "    negative_sample = []\n",
    "    # word index of target word:\n",
    "    i = word_indx[w]\n",
    "    # for each context word of word i, choose m negative samples\n",
    "    for context in context_dict[i]:\n",
    "        #shuffle A\n",
    "        np.random.shuffle(A)\n",
    "        # randomly choose samples != index of context word in A\n",
    "        index_selected = np.random.choice(np.where(A!= context)[0], size = m)\n",
    "        neg_words = list(A[index_selected].ravel())\n",
    "        negative_sample = negative_sample + neg_words \n",
    "        \n",
    "    \n",
    "    return negative_sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "advance-irrigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 654.0, 373.0, 2.0, 300.0, 2.0, 654.0, 2.0, 859.0, 654.0, 2.0, 937.0]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For examples, get the negative samples of word gains, select 4 negative words for each context word:\n",
    "neg_sample('gains', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "polyphonic-edmonton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[650, 783, 887]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check if the function above work well, check the index of word \"gains\" \n",
    "#then check the number of its context words:\n",
    "gain_indx = word_indx['gains']\n",
    "context_dict[gain_indx ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-pound",
   "metadata": {},
   "source": [
    "The word \"gains\" has 3 context words and funtion neg_samples( ) generate 12 negative samples. Thus, funtion neg_samples( ) works well  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-lesbian",
   "metadata": {},
   "source": [
    "defaultdict(set,\n",
    "            {10: {2, 6, 8, 9, 11},\n",
    "             8: {0, 1, 2, 5, 7, 9, 10, 11},\n",
    "             9: {4, 7, 8, 10},\n",
    "             7: {3, 4, 8, 9},\n",
    "             4: {3, 6, 7, 9},\n",
    "             3: {2, 4, 6, 7},\n",
    "             6: {2, 3, 4, 10},\n",
    "             2: {3, 6, 8, 10},\n",
    "             11: {0, 5, 8, 10},\n",
    "             5: {0, 8, 11},\n",
    "             0: {1, 5, 8, 11},\n",
    "             1: {0, 8}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "careful-classics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-theorem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
